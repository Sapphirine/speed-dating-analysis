{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "df = pd.read_csv('speed_dating_data.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5561, 73)\n"
     ]
    }
   ],
   "source": [
    "data1 = pd.concat([#df.iloc[:,0], #ID\n",
    "                  df.iloc[:,2], #gender\n",
    "                  df.iloc[:,9:10], #order of date\n",
    "                  df.iloc[:, 11:12], #partner's id\n",
    "                  df.iloc[:, 12:13], #match, \n",
    "                  df.iloc[:,13:17], #int_corr, samerace, age of partner, race of partner \n",
    "                  df.iloc[:,17:23], #stated preferences\n",
    "                  df.iloc[:,23:24], #decision of partner - dec-o\n",
    "                  df.iloc[:,24:32],   # rating by partner for 6 attributes, probability of matching with partner #amb-o has lots of missing values - try eliminating\n",
    "                  #df.iloc[:,32:35], # met, age, field\n",
    "                  df.iloc[:,39:42],#race, importance of race, importance of religion\n",
    "                  #df.iloc[:,42:43], #from   (DROPPED ZIPCODE AND INCOME AS THEY HAVE TOO MANY MISSING VALUES)\n",
    "                  df.iloc[:,45:48], #goal, date, go_out\n",
    "                  #df.iloc[:,48:50], #career, career_coded\n",
    "                  df.iloc[:,50:67], #interests\n",
    "                  #df.iloc[:,67:69], #exphappy, expnum (DROPPED - TOO MANY MISSING VALUES)\n",
    "                  df.iloc[:,69:74],  #what subject looks for in the opposite sex - 6 attributes\n",
    "                  df.iloc[:,74:75],  #shar1_1\n",
    "                  df.iloc[:,81:87], #attr2_1,..shar2_1\n",
    "                  df.iloc[:,87:92],  #attr3_1.. #91 is amb3_1 (how do you measure up)\n",
    "                  df.iloc[:,97:98], #decision of subject - dec\n",
    "                  df.iloc[:,98:102], #ratings by subject for partner for the 6 attributes. 101 is fun\n",
    "                  df.iloc[:,102:104], #amb,shar\n",
    "                  df.iloc[:,104:107]],axis=1) #like, prob, met\n",
    "\n",
    "data1 = data1.dropna()\n",
    "print(np.shape(data1))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>dec</td>       <th>  R-squared:         </th> <td>   0.635</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.631</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   142.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 11 Dec 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>00:49:13</td>     <th>  Log-Likelihood:    </th> <td> -2788.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  5561</td>      <th>  AIC:               </th> <td>   5712.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  5494</td>      <th>  BIC:               </th> <td>   6155.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    67</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gender</th>   <td>    0.0620</td> <td>    0.018</td> <td>    3.391</td> <td> 0.001</td> <td>    0.026</td> <td>    0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>order</th>    <td>    0.0009</td> <td>    0.001</td> <td>    0.868</td> <td> 0.386</td> <td>   -0.001</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>int_corr</th> <td>    0.0001</td> <td>    0.020</td> <td>    0.005</td> <td> 0.996</td> <td>   -0.038</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>samerace</th> <td>    0.0060</td> <td>    0.012</td> <td>    0.494</td> <td> 0.622</td> <td>   -0.018</td> <td>    0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age_o</th>    <td>   -0.0021</td> <td>    0.002</td> <td>   -1.277</td> <td> 0.202</td> <td>   -0.005</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>race_o</th>   <td>   -0.0026</td> <td>    0.005</td> <td>   -0.558</td> <td> 0.577</td> <td>   -0.012</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pf_o_att</th> <td>   -0.0001</td> <td>    0.002</td> <td>   -0.062</td> <td> 0.950</td> <td>   -0.004</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pf_o_sin</th> <td>   -0.0024</td> <td>    0.002</td> <td>   -1.061</td> <td> 0.289</td> <td>   -0.007</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pf_o_int</th> <td>   -0.0007</td> <td>    0.002</td> <td>   -0.312</td> <td> 0.755</td> <td>   -0.005</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pf_o_fun</th> <td>   -0.0016</td> <td>    0.002</td> <td>   -0.673</td> <td> 0.501</td> <td>   -0.006</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pf_o_amb</th> <td>   -0.0024</td> <td>    0.002</td> <td>   -1.088</td> <td> 0.277</td> <td>   -0.007</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pf_o_sha</th> <td>   -0.0028</td> <td>    0.002</td> <td>   -1.195</td> <td> 0.232</td> <td>   -0.007</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>attr_o</th>   <td>   -0.0175</td> <td>    0.004</td> <td>   -4.355</td> <td> 0.000</td> <td>   -0.025</td> <td>   -0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sinc_o</th>   <td>    0.0100</td> <td>    0.004</td> <td>    2.247</td> <td> 0.025</td> <td>    0.001</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intel_o</th>  <td>    0.0035</td> <td>    0.006</td> <td>    0.641</td> <td> 0.521</td> <td>   -0.007</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fun_o</th>    <td>   -0.0026</td> <td>    0.004</td> <td>   -0.596</td> <td> 0.551</td> <td>   -0.011</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>amb_o</th>    <td>    0.0043</td> <td>    0.004</td> <td>    1.046</td> <td> 0.296</td> <td>   -0.004</td> <td>    0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>shar_o</th>   <td>   -0.0065</td> <td>    0.004</td> <td>   -1.744</td> <td> 0.081</td> <td>   -0.014</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>like_o</th>   <td>   -0.0002</td> <td>    0.005</td> <td>   -0.043</td> <td> 0.965</td> <td>   -0.010</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>prob_o</th>   <td>    0.0110</td> <td>    0.003</td> <td>    3.552</td> <td> 0.000</td> <td>    0.005</td> <td>    0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>race</th>     <td>    0.0102</td> <td>    0.005</td> <td>    2.051</td> <td> 0.040</td> <td>    0.000</td> <td>    0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>imprace</th>  <td>   -0.0057</td> <td>    0.002</td> <td>   -2.431</td> <td> 0.015</td> <td>   -0.010</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>imprelig</th> <td>   -0.0057</td> <td>    0.002</td> <td>   -2.426</td> <td> 0.015</td> <td>   -0.010</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>goal</th>     <td>   -0.0075</td> <td>    0.004</td> <td>   -1.794</td> <td> 0.073</td> <td>   -0.016</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>date</th>     <td>   -0.0068</td> <td>    0.004</td> <td>   -1.575</td> <td> 0.115</td> <td>   -0.015</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>go_out</th>   <td>    0.0076</td> <td>    0.006</td> <td>    1.348</td> <td> 0.178</td> <td>   -0.003</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sports</th>   <td>   -0.0086</td> <td>    0.003</td> <td>   -2.919</td> <td> 0.004</td> <td>   -0.014</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tvsports</th> <td>    0.0014</td> <td>    0.003</td> <td>    0.545</td> <td> 0.586</td> <td>   -0.004</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>exercise</th> <td>   -0.0066</td> <td>    0.003</td> <td>   -2.479</td> <td> 0.013</td> <td>   -0.012</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dining</th>   <td>   -0.0058</td> <td>    0.004</td> <td>   -1.458</td> <td> 0.145</td> <td>   -0.014</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>museums</th>  <td>   -0.0132</td> <td>    0.006</td> <td>   -2.292</td> <td> 0.022</td> <td>   -0.024</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>art</th>      <td>    0.0104</td> <td>    0.005</td> <td>    2.058</td> <td> 0.040</td> <td>    0.000</td> <td>    0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hiking</th>   <td>   -0.0036</td> <td>    0.002</td> <td>   -1.471</td> <td> 0.141</td> <td>   -0.008</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gaming</th>   <td>    0.0071</td> <td>    0.003</td> <td>    2.815</td> <td> 0.005</td> <td>    0.002</td> <td>    0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>clubbing</th> <td>    0.0050</td> <td>    0.002</td> <td>    2.114</td> <td> 0.035</td> <td>    0.000</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reading</th>  <td>    0.0037</td> <td>    0.003</td> <td>    1.202</td> <td> 0.230</td> <td>   -0.002</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tv</th>       <td>   -0.0013</td> <td>    0.003</td> <td>   -0.436</td> <td> 0.663</td> <td>   -0.007</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>theater</th>  <td>    0.0044</td> <td>    0.004</td> <td>    1.233</td> <td> 0.218</td> <td>   -0.003</td> <td>    0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>movies</th>   <td>   -0.0113</td> <td>    0.004</td> <td>   -2.681</td> <td> 0.007</td> <td>   -0.020</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>concerts</th> <td>    0.0084</td> <td>    0.004</td> <td>    2.150</td> <td> 0.032</td> <td>    0.001</td> <td>    0.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>music</th>    <td>   -0.0093</td> <td>    0.004</td> <td>   -2.124</td> <td> 0.034</td> <td>   -0.018</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>shopping</th> <td>   -0.0041</td> <td>    0.003</td> <td>   -1.429</td> <td> 0.153</td> <td>   -0.010</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>yoga</th>     <td>    0.0081</td> <td>    0.002</td> <td>    3.516</td> <td> 0.000</td> <td>    0.004</td> <td>    0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>attr1_1</th>  <td>   -0.0070</td> <td>    0.002</td> <td>   -2.967</td> <td> 0.003</td> <td>   -0.012</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sinc1_1</th>  <td>   -0.0021</td> <td>    0.002</td> <td>   -0.867</td> <td> 0.386</td> <td>   -0.007</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intel1_1</th> <td>   -0.0045</td> <td>    0.002</td> <td>   -1.865</td> <td> 0.062</td> <td>   -0.009</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fun1_1</th>   <td>   -0.0019</td> <td>    0.002</td> <td>   -0.783</td> <td> 0.434</td> <td>   -0.007</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>amb1_1</th>   <td>   -0.0058</td> <td>    0.002</td> <td>   -2.566</td> <td> 0.010</td> <td>   -0.010</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>shar1_1</th>  <td>   -0.0019</td> <td>    0.002</td> <td>   -0.797</td> <td> 0.426</td> <td>   -0.007</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>attr2_1</th>  <td>    0.0042</td> <td>    0.003</td> <td>    1.615</td> <td> 0.106</td> <td>   -0.001</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sinc2_1</th>  <td>    0.0050</td> <td>    0.003</td> <td>    1.875</td> <td> 0.061</td> <td>   -0.000</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intel2_1</th> <td>    0.0046</td> <td>    0.003</td> <td>    1.649</td> <td> 0.099</td> <td>   -0.001</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fun2_1</th>   <td>    0.0027</td> <td>    0.003</td> <td>    0.985</td> <td> 0.325</td> <td>   -0.003</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>amb2_1</th>   <td>    0.0036</td> <td>    0.003</td> <td>    1.352</td> <td> 0.177</td> <td>   -0.002</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>shar2_1</th>  <td>    0.0037</td> <td>    0.003</td> <td>    1.340</td> <td> 0.180</td> <td>   -0.002</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>attr3_1</th>  <td>   -0.0128</td> <td>    0.005</td> <td>   -2.423</td> <td> 0.015</td> <td>   -0.023</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sinc3_1</th>  <td>   -0.0233</td> <td>    0.005</td> <td>   -5.064</td> <td> 0.000</td> <td>   -0.032</td> <td>   -0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fun3_1</th>   <td>   -0.0150</td> <td>    0.005</td> <td>   -3.075</td> <td> 0.002</td> <td>   -0.025</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intel3_1</th> <td>    0.0217</td> <td>    0.006</td> <td>    3.431</td> <td> 0.001</td> <td>    0.009</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>amb3_1</th>   <td>    0.0121</td> <td>    0.004</td> <td>    3.101</td> <td> 0.002</td> <td>    0.004</td> <td>    0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>attr</th>     <td>    0.0766</td> <td>    0.004</td> <td>   20.619</td> <td> 0.000</td> <td>    0.069</td> <td>    0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sinc</th>     <td>   -0.0151</td> <td>    0.005</td> <td>   -3.343</td> <td> 0.001</td> <td>   -0.024</td> <td>   -0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intel</th>    <td>    0.0113</td> <td>    0.006</td> <td>    2.025</td> <td> 0.043</td> <td>    0.000</td> <td>    0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fun</th>      <td>    0.0320</td> <td>    0.004</td> <td>    7.616</td> <td> 0.000</td> <td>    0.024</td> <td>    0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>amb</th>      <td>   -0.0180</td> <td>    0.004</td> <td>   -4.291</td> <td> 0.000</td> <td>   -0.026</td> <td>   -0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>shar</th>     <td>    0.0317</td> <td>    0.004</td> <td>    8.810</td> <td> 0.000</td> <td>    0.025</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>prob</th>     <td>    0.0404</td> <td>    0.003</td> <td>   12.726</td> <td> 0.000</td> <td>    0.034</td> <td>    0.047</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>942.749</td> <th>  Durbin-Watson:     </th> <td>   1.591</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 208.886</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.054</td>  <th>  Prob(JB):          </th> <td>4.37e-46</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.057</td>  <th>  Cond. No.          </th> <td>    326.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    dec   R-squared:                       0.635\n",
       "Model:                            OLS   Adj. R-squared:                  0.631\n",
       "Method:                 Least Squares   F-statistic:                     142.9\n",
       "Date:                Wed, 11 Dec 2019   Prob (F-statistic):               0.00\n",
       "Time:                        00:49:13   Log-Likelihood:                -2788.8\n",
       "No. Observations:                5561   AIC:                             5712.\n",
       "Df Residuals:                    5494   BIC:                             6155.\n",
       "Df Model:                          67                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "gender         0.0620      0.018      3.391      0.001       0.026       0.098\n",
       "order          0.0009      0.001      0.868      0.386      -0.001       0.003\n",
       "int_corr       0.0001      0.020      0.005      0.996      -0.038       0.039\n",
       "samerace       0.0060      0.012      0.494      0.622      -0.018       0.030\n",
       "age_o         -0.0021      0.002     -1.277      0.202      -0.005       0.001\n",
       "race_o        -0.0026      0.005     -0.558      0.577      -0.012       0.006\n",
       "pf_o_att      -0.0001      0.002     -0.062      0.950      -0.004       0.004\n",
       "pf_o_sin      -0.0024      0.002     -1.061      0.289      -0.007       0.002\n",
       "pf_o_int      -0.0007      0.002     -0.312      0.755      -0.005       0.004\n",
       "pf_o_fun      -0.0016      0.002     -0.673      0.501      -0.006       0.003\n",
       "pf_o_amb      -0.0024      0.002     -1.088      0.277      -0.007       0.002\n",
       "pf_o_sha      -0.0028      0.002     -1.195      0.232      -0.007       0.002\n",
       "attr_o        -0.0175      0.004     -4.355      0.000      -0.025      -0.010\n",
       "sinc_o         0.0100      0.004      2.247      0.025       0.001       0.019\n",
       "intel_o        0.0035      0.006      0.641      0.521      -0.007       0.014\n",
       "fun_o         -0.0026      0.004     -0.596      0.551      -0.011       0.006\n",
       "amb_o          0.0043      0.004      1.046      0.296      -0.004       0.012\n",
       "shar_o        -0.0065      0.004     -1.744      0.081      -0.014       0.001\n",
       "like_o        -0.0002      0.005     -0.043      0.965      -0.010       0.010\n",
       "prob_o         0.0110      0.003      3.552      0.000       0.005       0.017\n",
       "race           0.0102      0.005      2.051      0.040       0.000       0.020\n",
       "imprace       -0.0057      0.002     -2.431      0.015      -0.010      -0.001\n",
       "imprelig      -0.0057      0.002     -2.426      0.015      -0.010      -0.001\n",
       "goal          -0.0075      0.004     -1.794      0.073      -0.016       0.001\n",
       "date          -0.0068      0.004     -1.575      0.115      -0.015       0.002\n",
       "go_out         0.0076      0.006      1.348      0.178      -0.003       0.019\n",
       "sports        -0.0086      0.003     -2.919      0.004      -0.014      -0.003\n",
       "tvsports       0.0014      0.003      0.545      0.586      -0.004       0.007\n",
       "exercise      -0.0066      0.003     -2.479      0.013      -0.012      -0.001\n",
       "dining        -0.0058      0.004     -1.458      0.145      -0.014       0.002\n",
       "museums       -0.0132      0.006     -2.292      0.022      -0.024      -0.002\n",
       "art            0.0104      0.005      2.058      0.040       0.000       0.020\n",
       "hiking        -0.0036      0.002     -1.471      0.141      -0.008       0.001\n",
       "gaming         0.0071      0.003      2.815      0.005       0.002       0.012\n",
       "clubbing       0.0050      0.002      2.114      0.035       0.000       0.010\n",
       "reading        0.0037      0.003      1.202      0.230      -0.002       0.010\n",
       "tv            -0.0013      0.003     -0.436      0.663      -0.007       0.005\n",
       "theater        0.0044      0.004      1.233      0.218      -0.003       0.012\n",
       "movies        -0.0113      0.004     -2.681      0.007      -0.020      -0.003\n",
       "concerts       0.0084      0.004      2.150      0.032       0.001       0.016\n",
       "music         -0.0093      0.004     -2.124      0.034      -0.018      -0.001\n",
       "shopping      -0.0041      0.003     -1.429      0.153      -0.010       0.002\n",
       "yoga           0.0081      0.002      3.516      0.000       0.004       0.013\n",
       "attr1_1       -0.0070      0.002     -2.967      0.003      -0.012      -0.002\n",
       "sinc1_1       -0.0021      0.002     -0.867      0.386      -0.007       0.003\n",
       "intel1_1      -0.0045      0.002     -1.865      0.062      -0.009       0.000\n",
       "fun1_1        -0.0019      0.002     -0.783      0.434      -0.007       0.003\n",
       "amb1_1        -0.0058      0.002     -2.566      0.010      -0.010      -0.001\n",
       "shar1_1       -0.0019      0.002     -0.797      0.426      -0.007       0.003\n",
       "attr2_1        0.0042      0.003      1.615      0.106      -0.001       0.009\n",
       "sinc2_1        0.0050      0.003      1.875      0.061      -0.000       0.010\n",
       "intel2_1       0.0046      0.003      1.649      0.099      -0.001       0.010\n",
       "fun2_1         0.0027      0.003      0.985      0.325      -0.003       0.008\n",
       "amb2_1         0.0036      0.003      1.352      0.177      -0.002       0.009\n",
       "shar2_1        0.0037      0.003      1.340      0.180      -0.002       0.009\n",
       "attr3_1       -0.0128      0.005     -2.423      0.015      -0.023      -0.002\n",
       "sinc3_1       -0.0233      0.005     -5.064      0.000      -0.032      -0.014\n",
       "fun3_1        -0.0150      0.005     -3.075      0.002      -0.025      -0.005\n",
       "intel3_1       0.0217      0.006      3.431      0.001       0.009       0.034\n",
       "amb3_1         0.0121      0.004      3.101      0.002       0.004       0.020\n",
       "attr           0.0766      0.004     20.619      0.000       0.069       0.084\n",
       "sinc          -0.0151      0.005     -3.343      0.001      -0.024      -0.006\n",
       "intel          0.0113      0.006      2.025      0.043       0.000       0.022\n",
       "fun            0.0320      0.004      7.616      0.000       0.024       0.040\n",
       "amb           -0.0180      0.004     -4.291      0.000      -0.026      -0.010\n",
       "shar           0.0317      0.004      8.810      0.000       0.025       0.039\n",
       "prob           0.0404      0.003     12.726      0.000       0.034       0.047\n",
       "==============================================================================\n",
       "Omnibus:                      942.749   Durbin-Watson:                   1.591\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              208.886\n",
       "Skew:                           0.054   Prob(JB):                     4.37e-46\n",
       "Kurtosis:                       2.057   Cond. No.                         326.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Baseline regression model (without any feature selection -  contains 67 predictors)\n",
    "#these 67 features account for 65 % of the variance in the dataset for y = dec\n",
    "#TODO : replace dec with match and dec_o \n",
    "Y_reg = data1.dec\n",
    "X_reg1 = data1[['gender','order','int_corr','samerace','age_o','race_o', 'pf_o_att','pf_o_sin', 'pf_o_int', 'pf_o_fun', 'pf_o_amb','pf_o_sha','attr_o','sinc_o','intel_o','fun_o','amb_o','shar_o','like_o','prob_o','race','imprace','imprelig','goal','date','go_out','sports','tvsports','exercise','dining','museums','art','hiking','gaming','clubbing','reading','tv','theater','movies','concerts','music','shopping','yoga','attr1_1','sinc1_1','intel1_1','fun1_1','amb1_1','shar1_1','attr2_1','sinc2_1','intel2_1','fun2_1','amb2_1','shar2_1','attr3_1','sinc3_1','fun3_1','intel3_1','amb3_1','attr','sinc','intel','fun','amb','shar','prob']]\n",
    "model = sm.OLS(Y_reg, X_reg1)\n",
    "result_reg = model.fit()\n",
    "result_reg.summary()\n",
    "\n",
    "#results : p-values imply only few variables are statistically significant. F-statistic value is also less.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "#Recursive feature elimination - sending a subset of previous features\n",
    "y = data1.dec\n",
    "X = data1[['gender','attr_o','sinc_o','intel_o','fun_o','amb_o','shar_o','prob_o','race','imprace','imprelig','goal','attr1_1','sinc1_1','intel1_1','fun1_1','amb1_1','shar1_1','attr2_1','sinc2_1','intel2_1','fun2_1','amb2_1','shar2_1','attr3_1','sinc3_1','fun3_1','intel3_1','amb3_1','attr','sinc','intel','fun','amb','shar','prob']]\n",
    "estimator = SVR(kernel=\"linear\")\n",
    "selector = RFE(estimator, 20, step=1)\n",
    "selector = selector.fit(X, y)\n",
    "selector.ranking_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.support_\n",
    "#method selects following 20 best features from the above subset of features\n",
    "#gender, attr_o, sinc_o, intel_o, shar_o, prob_o, race, imprace, goal, attr3_1, sinc3_1, fun3_1, intel3_1, amb3_1, attr, sinc, fun, amb, shar, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the above features after feature selection, regression model gives better values. p-values < 0.0001 - most of these features are statistically significant\n",
    "#Use these features in classification \n",
    "\n",
    "#TODO: feature selection with y = match\n",
    "\n",
    "y = data1.dec\n",
    "X_reg4 = data1[['gender','attr_o','sinc_o','shar_o','prob_o','race','imprace','goal','attr3_1','sinc3_1','fun3_1','intel3_1','amb3_1','attr','sinc','fun','amb','shar','prob']]\n",
    "\n",
    "model = sm.OLS(y, X_reg4)\n",
    "result_reg = model.fit()\n",
    "result_reg.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
